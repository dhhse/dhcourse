{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Reading,переходим к Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как использовать этот код \n",
    "### (можно пропустить эту часть, если вы знаете, что такое Jupyter и ipynb-тетрадки -- в таком случае переходите сразу к \"Начало работы\")\n",
    "То, на что вы сейчас смотрите — тетрадка Jupyter Notebook. Это одна из популярных сред для написания и __демонстрации__ кода на Python (и не только). Jupyter запускает питоновский код прямо в браузере (но локально, т.е. код исполняет ваш компьютер, в отличие от, например Google Colab). В Jupyter код можно запускать не целиком, а по ячейкам (в отличие от IDLE). \n",
    "\n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ячейка с кодом\n",
    "text = 'Мы с Даней сегодня тестим инструменты дистант ридинга'\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# еще одна ячейка с кодом. \n",
    "# Пространство имен общее — переменная text была заполнена в предыдущей ячейке\n",
    "print (text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, ячейки бывают текстовыми — их можно форматировать с помощью формата разметки markdown (.md). Текст, который вы сейчас читаете, а также всякие описания и картинки в предыдущей части этого занятия, написаны как раз в таком формате. Файлы Jupyter Notebook имеют расширение .ipynb и автоматически рендерятся гитхабом. Например, вот эта __[тетрадка](https://github.com/DanilSko/itmo/blob/master/ITMO_practice/BasicProcessing/Basic%20text%20processing%20Danya%20ITMO.ipynb)__  у меня на гитхабе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Окей, а как я могу запустить код в таком .ipynb?\n",
    "\n",
    "Есть разные варианты: \n",
    "\n",
    "### Через __[Google Colab](https://colab.research.google.com)__. \n",
    "\n",
    "Google Colab — это гугловский инструмент для написания кода в браузере и запуска прямо на серверах Google (с возможностью бесплатно использовать их вычислительные мощности, в т.ч. графические процессор  — GPU).  Google Colab — родственник Jupyter, они очень похожи. Разница в том, что Jupyter работает локально и использует ваш собственный питон, а Google Colab — это облачный сервис, похожий на Google Docs: вы можете делиться тетрадками и т.д. В Colab можно открыть эту тетрадку, указав ссылку на нее: \n",
    "\n",
    "![Colab](pics/github2colab.png) \n",
    "\n",
    "Просто загрузить .ipynb-файл в Colab тоже можно. \n",
    "\n",
    "### Скачать себе .ipynb и открыть в Jupyter\n",
    "\n",
    "Скачать .ipynb с гитхаба: \n",
    "\n",
    "![Raw](pics/raw_download.png) \n",
    "\n",
    "Поставить Jupyter Notebook по инструкции __[отсюда](https://jupyter.readthedocs.io/en/latest/install.html)__. Открыть терминал (в Windows — командную строку).пойти в папку, внутри которой лежит скачанный .ipynb, написать там jupyter notebook. После этого у вас должно открыться в браузере что-то такое:\n",
    "\n",
    "![Colab](pics/jupyter2.png)\n",
    "\n",
    "Тыкайте на тетрадку -- и все, можно работать в ней, писать код и т.п. Из Jupyter можно выгрузить и отдельно код в виде файла .py (но тогда разделение на ячейки пропадет).\n",
    "\n",
    "### Скачать .ipynb и открыть в PyCharm\n",
    "Если вы любите популярную у питонистов IDE PyCharm  -- она умеет открывать ipynb. Но для этого все равно нужен установеленный Jupyter. Поэтому надо проделать все то же, что в предыд.пункте, а потом открыть файл в PyCharm.\n",
    "\n",
    "### (хак) Скачать файл .py из Google Colab или Jupyter и открыть в PyCharm или любой IDE\n",
    "Из Colab после создания копии вы можете выгрузить не весь .ipynb, а только код в .py. Этот код уже можно запускать где угодно, хоть из командной строки (терминала) вашего компьютера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Начало работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# проверим, что рядом в папке лежит файл с Преступлением и наказанием.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## У тех, кто смотрит в колабе, его, естественно, не будет. Поэтому вот стандартный код для загрузки в колаб\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __[Ссылка на текстовый файл с Преступлением и наказанием](https://github.com/DanilSko/itmo/blob/master/ITMO_practice/BasicProcessing/Dostoevsky_PrestuplenieINakazanie.txt)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# откроем файл в питоне\n",
    "path_to_file = 'whole_corpus.txt'\n",
    "prest_i_nak = open (path_to_file, 'r')\n",
    "prest_i_nak_kak_stroka = prest_i_nak.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## попробуйте вывести тут длину строки с помощью функции len; надо написать len (prest_i_nak_kak_stroka) -- так вы передадите в функцию len ваш текст, считанный в сторку \n",
    "len (prest_i_nak_kak_stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## попробуйте получить срез из первых 100 символов вашей строки с помощью срезов списка [:]\n",
    "## т.е. напишите ниже вот это: prest_i_nak_kak_stroka [:100]\n",
    "## и нажмите выполнить\n",
    "prest_i_nak_kak_stroka [-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Простейший способ сегментации строки на токены (как бы на слова, но тупее грубее) — питоновский встроенный метод .split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_kak_spisok_slov = prest_i_nak_kak_stroka.split() ## здесь в качестве аргумента можно воткнуть разделитель; по умолчанию это любое количество пробелов\n",
    "#print ('Примерное количество слов в \"Преступлении и наказании\":', len (prest_i_nak_kak_spisok_slov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'I love ITMO'.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_kak_spisok_slov[:200]\n",
    "\n",
    "#type (prest_i_nak_kak_stroka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### попробуем получить список уникальных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_kak_mnojestvo = set(prest_i_nak_kak_spisok_slov)\n",
    "len (prest_i_nak_kak_mnojestvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_kak_mnojestvo_slov = set (prest_i_nak_kak_spisok_slov)\n",
    "len (prest_i_nak_kak_mnojestvo_slov)\n",
    "prest_i_nak_kak_spisok_unikalnyh_slov = list (prest_i_nak_kak_mnojestvo_slov)\n",
    "(prest_i_nak_kak_spisok_unikalnyh_slov).sort()\n",
    "prest_i_nak_kak_spisok_unikalnyh_slov [:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Более умный способ: сегментируем текст регекспом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## питоновский модуль для регекспов\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_spisok_re =  re.split ('( +|[.,!? –]|\\n)',prest_i_nak_kak_stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_spisok_re [:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Еще более умный способ: сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# если у вас еще нет nltk, установите его:\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "clean_prest_i_nak_kak_stroka = re.sub ('[.,?!-:]', '', prest_i_nak_kak_stroka)\n",
    "prest_i_nak_nltk_tokenized = word_tokenize (clean_prest_i_nak_kak_stroka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_nltk_tokenized [:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А теперь давайте посчитаем частотность слов в нашем списке слов; т.е. собственно как часто повторяется каждое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## для этого мы тоже не станем писать свое решение с нуля, а воспользуемся готовым от NLTK\n",
    "## возьеме токенизированный список — и засунем его в функцию FreqDist от NLTK\n",
    "from nltk import FreqDist\n",
    "word_freqs_prest_i_nak = FreqDist(prest_i_nak_nltk_tokenized)\n",
    "word_freqs_prest_i_nak.most_common (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_freqs_prest_i_nak.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стемминг, лемматизация, морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В том же NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## стеммер не быстрый, поэтому давайте возьмем первые 1000 слов, а не все\n",
    "first_1000 = prest_i_nak_nltk_tokenized[:1000]\n",
    "## этот стеммер не умеет сам токенизировать -- он работает только с отдельными словами. \n",
    "## Поэтому придется скармливать ему наш список по одному: \n",
    "prest_i_nak_first_1000_stemmed = []\n",
    "for word in first_1000:\n",
    "    prest_i_nak_first_1000_stemmed.append (stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_stemmer = SnowballStemmer(\"english\")\n",
    "string = 'I love teaching students at ITMO'\n",
    "tostem = string.split()\n",
    "for word in tostem:\n",
    "    print (en_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prest_i_nak_first_1000_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyStem\n",
    "\n",
    "__[Mystem](https://tech.yandex.ru/mystem/)__ - это свободно распространяемый морфологический анализатор для русского языка с закрытым исходным кодом. То есть мы можем его бесплатно скачать с сайта и пользоваться им, но не можем посмотреть, что у него внутри и как оно работает.\n",
    "\n",
    "Mystem был придуман одним из создателей Яндекса Ильёй Сегаловичем. Некоторый потомок Mystem'а до сих пор работает внутри большого поисковика Яндекса, анализируя слова при поиске.\n",
    "\n",
    "MyStem значит my stemmer. Как мы с вами уже знаем (см. выше), стемминг -- это разбиение формы на основу и флексию. Программы-стеммеры умеют превращать фразу 'Маша поехала за грибами' в 'Маш поехал за гриб'.  Но на самом деле MyStem не стеммер, а полноценный морфологический АНАЛИЗАТОР. Он может гораздо больше: устанавливать словарную форму слова, определять часть речи и грамматические характеристики (падеж, число, время, род...). В последних версиях Mystem умеет и выбирать из нескольких возможных грамматических разборов один, наиболее верный.\n",
    "\n",
    "У Mystem нет графического оконного интерфейса, запустить его можно только из командной строки. Зато есть обертка для Python — pymystem3.\n",
    "\n",
    "__[Документация к MyStem](https://tech.yandex.ru/mystem/doc/index-docpage/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# эта клетка нужна вам, если у вас не установлен модуль pymystem3. \n",
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## импортируем непосредственно класс \"анализатор MyStem\" из pymystem3\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help (Mystem()) ## всегда полезно почитать хелпы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moi_analizator = Mystem() ## создаем анализатор\n",
    "test = 'Даня тестирует машинную морфологию' ## создаем тестовую строку\n",
    "lemmatized = moi_analizator.lemmatize(test) ## лемматизируем строку с помощью mystem \n",
    "print (lemmatized) ## напечатаем лемматизированную строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prest_i_nak_by_mystem = moi_analizator.lemmatize(clean_prest_i_nak_kak_stroka) ## лемматизируем преступление и наказание с помощью mystem \n",
    "#print  (prest_i_nak_kak_by_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_freqs_prest_i_nak = FreqDist(prest_i_nak_by_mystem)\n",
    "word_freqs_prest_i_nak.most_common (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
