{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface\n",
    "В этом практикуме многое позаимствовано у моего коллеги, преподавателя компьютерной лингвистики Миши Нефедова. Спасибо ему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое извлечение ключевых слов\n",
    "\n",
    "Извлечение ключевых слов (keyword extraction) - способ извлечения информации из текста и анализа его тематики. В отличие от автоматического реферирования (саммаризации), мы не создаем полноценный пересказ, а вытаскиваем только ключевые слова (Keywords). Иногда идут чуть дальше и извлекают ключевые словосочетания/n-граммы (Keyphrases). Например, чтобы для текста про \"морских коров\" не извлекались слова \"корова\" и \"морской\", а извлекалось все словосочетание. Это еще актуальнее для более аналитических языков вроде английского (ср. \"хотдог\" - \"hot dog\"; вряд ли вы хотите ключевое слово \"собака\" в тексте про хотдоги)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем это нужно?\n",
    "\n",
    "Задача извлечения ключевых слов обычно решается в информационном поиске. Если вы решите писать свой поисковик, или просто индесировать какую-то большую коллекцию текстов, понимать эту тему необходимо. Но и для других задач это может быть полезно. В каком-то смысле, извлечение ключевых слов — простейший способ \"тематического моделирования\" корпуса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## К делу!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План такой: \n",
    "1. Подумать, какие подходы возможны. Реализовать их на случайном примере, оценить результаты глазами\n",
    "\n",
    "1. Взять готовый набор данных, где ключевые слова уже выделены людьми, и попробовать сравнить с ними (как с эталоном) результат наших методов. Важно помнить, что понятие \"эталона\" здесь условно. Задача keyword extraction допускает альтернативные решения для одного текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os ## работаем с файлами, значит, наверняка понадобится ос\n",
    "from nltk import word_tokenize ## работаем со словами — значит, токенизатор для этих файлов понадобится\n",
    "import string ## возьмем оттуда punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте возьмем для эксперимента 6 текстов: 4 новостных и 2 художественных. Скачать можно [тут](samples_for_class.zip). У меня они лежат в папке 'samples_for_class' рядом с кодом. Положу этот путь в переменную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_TEXTS = 'samples_for_class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запишу тексты всех файлов в один список, чтобы потом с ними легче работать: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## в этот пустой список я потом запишу тексты файлов, чтобы потом применять к ним разные методы:\n",
    "file_texts = []\n",
    "# в этом цикле я сложу в file_texts тексты файлов, лежащие по адресу в PATH_TO_TEXTS:\n",
    "for some_file in os.listdir (PATH_TO_TEXTS): \n",
    "    if some_file.endswith ('.txt'):\n",
    "        with open (os.path.join(PATH_TO_TEXTS, some_file),'r') as open_file:\n",
    "            file_texts.append (open_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пробуем подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самый тупой вариант: берем первые и последние слова текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем фукнцию, которая берет первые и последние слова (их число задается пользователем как параметр функции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = word_tokenize (some_text.lower())\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним по текстам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Глава', 'Федерального', 'медико-биологического', 'агентства', ',', 'тысяч', 'тестов', 'на', 'коронавирус', '.']\n",
      "['Перед', 'судебным', 'следователем', 'стоит', 'маленький', 'дело', ',', 'по', 'совести', '...']\n",
      "['Власти', 'разработали', 'два', 'стресс-сценария', 'для', 'утвержден', 'правительством', '10', 'апреля', '.']\n",
      "['Несколько', 'крупнейших', 'сетей', 'медицинских', 'лабораторий', 'этом', 'услуга', 'будет', 'платной', '.']\n",
      "['Россияне', 'почти', 'до', 'нуля', 'сократили', 'больше', 'покупок', 'совершают', 'онлайн', '.']\n",
      "['Через', 'базарную', 'площадь', 'идет', 'полицейский', 'путь', 'по', 'базарной', 'площади', '.']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast (text, 5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось не очень. А еще мешают знаки препинания... Давайте их отфильтруем (на этапе заполнения переменной tokenized_text. Я вставлю там генератор списка, в котором буду проверять, что помещаемый в список элемент не является пунктуатором). Сначала напишем это в виде цикла с if-ом внутри:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    # пройдемся циклом по результату токенизации текста\n",
    "    for word in word_tokenize(some_text.lower()):\n",
    "        # выцепим все слова, которые не входят в string.punctuation\n",
    "        if word not in string.punctuation:\n",
    "            #\n",
    "            tokenized_text.append (word)\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь сделаем этот код компактнее, использовав генератор списка, о которых как раз говорил вам Борис Валерьевич на последем занятии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = [word for word in word_tokenize(some_text.lower()) if word not in string.punctuation]\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Глава', 'Федерального', 'медико-биологического', 'агентства', 'бывший', '795', 'тысяч', 'тестов', 'на', 'коронавирус']\n",
      "['Перед', 'судебным', 'следователем', 'стоит', 'маленький', 'за', 'дело', 'по', 'совести', '...']\n",
      "['Власти', 'разработали', 'два', 'стресс-сценария', 'для', 'будет', 'утвержден', 'правительством', '10', 'апреля']\n",
      "['Несколько', 'крупнейших', 'сетей', 'медицинских', 'лабораторий', 'При', 'этом', 'услуга', 'будет', 'платной']\n",
      "['Россияне', 'почти', 'до', 'нуля', 'сократили', 'все', 'больше', 'покупок', 'совершают', 'онлайн']\n",
      "['Через', 'базарную', 'площадь', 'идет', 'полицейский', 'свой', 'путь', 'по', 'базарной', 'площади']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast_no_punct (text, 5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше, но в стандартную сборку string.punctuation явно входит не все, что нам нужно. Усовершенствуем ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_punctuation = string.punctuation + '—»«...' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем функцию, добавив туда extenвed_punctuation вместо string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = [word for word in word_tokenize(some_text.lower()) if word not in extended_punctuation]\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Глава', 'Федерального', 'медико-биологического', 'агентства', 'бывший', '795', 'тысяч', 'тестов', 'на', 'коронавирус']\n",
      "['Перед', 'судебным', 'следователем', 'стоит', 'маленький', 'чтоб', 'за', 'дело', 'по', 'совести']\n",
      "['Власти', 'разработали', 'два', 'стресс-сценария', 'для', 'будет', 'утвержден', 'правительством', '10', 'апреля']\n",
      "['Несколько', 'крупнейших', 'сетей', 'медицинских', 'лабораторий', 'При', 'этом', 'услуга', 'будет', 'платной']\n",
      "['Россияне', 'почти', 'до', 'нуля', 'сократили', 'все', 'больше', 'покупок', 'совершают', 'онлайн']\n",
      "['Через', 'базарную', 'площадь', 'идет', 'полицейский', 'свой', 'путь', 'по', 'базарной', 'площади']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast_no_punct (text, 5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй самый тупой вариант: берем самые частотные слова текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist # как вы помните, в нлтк есть счетчик частотностей\n",
    "# но можно и пользоваться Counter из модуля collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краткое напоминание: как работает FreqDist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'повар': 2, 'петр': 1, 'павел': 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'повар петр повар павел'\n",
    "FreqDist(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('повар', 2), ('петр', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(text.split()).most_common (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent (some_text, num_most_freq):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число самых частотных слов от начала num_most_freq\"\"\"\n",
    "    # запишем в переменную tokenized_text список токенов без пунктуации\n",
    "    tokenized_text = [word for word in word_tokenize (some_text.lower()) if word not in extended_punctuation] \n",
    "    # вернем самые частотные слова, посчитанные с помощью метода most_common у FreqDist \n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', 'на', 'скворцова', 'россии', 'по', 'до', 'что', 'covid-19', '10-14', 'дней']\n",
      "['и', 'не', 'на', 'а', 'ты', 'в', 'что', 'денис', 'то', 'с']\n",
      "['в', 'и', 'на', 'до', 'года', 'будет', 'предприятий', 'что', 'системообразующих', 'а']\n",
      "['в', 'на', 'и', 'тестирование', 'роспотребнадзора', 'коронавирус', 'только', 'тесты', 'что', 'регионах']\n",
      "['на', 'в', 'и', 'по', 'расходы', 'неделю', 'с', 'россияне', 'почти', 'до']\n",
      "['и', 'не', 'в', 'а', 'я', 'на', 'у', 'ты', 'это', 'очумелов']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent (text, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Третий вариант: берем самые частотные слова текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open ('stop_ru.txt', 'r') as stop_file:\n",
    "    rus_stops = [word.strip() for word in stop_file.readlines()] # запишем стослова в список "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop (some_text, num_most_freq, some_stoplist):\n",
    "    tokenized_text = [word for word in word_tokenize (some_text.lower()) if word not in extended_punctuation and word not in some_stoplist]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['скворцова', 'россии', 'covid-19', '10-14', 'дней', 'данным']\n",
      "['денис', 'грузила', 'следователь', 'гайку', 'нешто', 'благородие']\n",
      "['предприятий', 'системообразующих', '2020', 'минэкономразвития', 'компаний', 'нефть']\n",
      "['тестирование', 'роспотребнадзора', 'коронавирус', 'тесты', 'регионах', 'представитель']\n",
      "['расходы', 'неделю', 'россияне', 'салоны', 'красоты', 'сбербанка']\n",
      "['очумелов', 'собака', 'благородие', 'палец', 'ежели', 'собаку']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop (text, 6, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Четвертый вариант: берем самые частотные ЛЕММЫ текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem ## используем mystem в обертке pymystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moi_analizator = Mystem() ## создаем экземпляр класса \"анализатор MyStem\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def passed_filter (some_word, stoplist):\n",
    "    some_word = some_word.strip()\n",
    "    if some_word in extended_punctuation:\n",
    "        return False\n",
    "    elif some_word in stoplist:\n",
    "        return False\n",
    "    elif re.search ('[А-ЯЁа-яёA-Za-z]', some_word) == None:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop_and_lemm (some_text, num_most_freq, stoplist):\n",
    "    lemmatized_text = [word for word in moi_analizator.lemmatize(some_text.lower()) if passed_filter(word, stoplist)]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(lemmatized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['россия', 'скворцов', 'инфекция', '10', 'коронавирус', 'коронавирусный', 'covid', '19', '14', 'данные']\n",
      "['денис', 'гайка', 'отвинчивать', 'грузило', 'понимать', 'следователь', 'рельс', 'врать', 'знать', 'говорить']\n",
      "['год', 'стресс', 'сценарий', 'квартал', 'нефть', 'предприятие', '2020', 'конец', 'минпромторг', 'системообразующий']\n",
      "['тестирование', 'лаборатория', 'тест', 'роспотребнадзор', 'коронавирус', 'биоматериал', 'регион', '», ', 'частный', 'государственный']\n",
      "['неделя', 'расход', '%, ', 'россиянин', 'категория', 'товар', '9', '2', '», «', ' (+']\n",
      "['собака', 'очумел', 'палец', 'знать', 'толпа', 'благородие', '!..', 'спрашивать', 'ежели', 'городовой']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop_and_lemm (text, 10, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А какие-нибудь более умные варианты будут?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Meet TF-IDF ! \n",
    "\n",
    "TF IDF — это мера, которая учитывает не только частотность слова в документе (TF, term frequency) — но и то, насколько часто — вернее, насколько редко! — оно встречается во всем корпусе (IDF, inverse document frequency). Формально она считается так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/tfidf.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть в том, что таким образом повышаются слова, частотные в данном документе -- и редкие в корпусе в целом. Такие слова получают бонус относительно слов, частотных и в данном документе, и в других. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer ## готовая реализация TF IDF из библиотеки sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_tf_idf = TfidfVectorizer (stop_words=rus_stops) # Создаем специальный объект-векторайзер \n",
    "#fitted_vectorizer = make_tf_idf.fit(file_texts) \n",
    "texts_as_tfidf_vectors=make_tf_idf.fit_transform(file_texts) # Кладем в этот векторайзер наши файлы и просим сделать матрицу TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_as_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (texts_as_tfidf_vectors.shape) ## посмотрим размерность матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_tf_idf.get_feature_names()[100:200] ## посмотрим "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['скворцова', '10', 'россии', 'данным', 'коронавируса']\n",
      "['денис', 'грузила', 'следователь', 'гайку', 'ежели']\n",
      "['стресс', '2020', 'предприятий', 'минэкономразвития', 'системообразующих']\n",
      "['тестирование', 'роспотребнадзора', 'коронавирус', 'регионах', 'представитель']\n",
      "['расходы', 'неделю', 'россияне', 'салоны', 'сбербанка']\n",
      "['очумелов', 'собака', 'палец', 'благородие', 'собаку']\n"
     ]
    }
   ],
   "source": [
    "## кусочек кода, который берет матрицу TF-IDF и выдает по ней топ-слова для каждого текста\n",
    "\n",
    "id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "    row_data = texts_as_tfidf_vectors.getrow(text_row) ## берем ряд в нашей матрице -- он соответстует тексту\n",
    "    words_for_this_text = row_data.toarray().argsort() ## сортируем в нем все слова (вернее, индексы слов) -- получаем от самых маленьких к самым большим\n",
    "    top_words_for_this_text = words_for_this_text [0,:-6:-1] ## берем крайние слова отсортированного ряда\n",
    "    print ([id2word[w] for w in top_words_for_this_text]) ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Естественно, тут можно снова накрутить лемматизацию "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## пусть лемматизируется в отдельной функции \n",
    "def preprocess_for_tfidif (some_text):\n",
    "    lemmatized_text = moi_analizator.lemmatize(some_text.lower())\n",
    "    return (' '.join(lemmatized_text)) # поскольку tfidf векторайзер принимает на вход строку, \n",
    "    #после лемматизации склеим все обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_tf_idf_keywords (some_texts, number_of_words):\n",
    "    make_tf_idf = TfidfVectorizer (stop_words=rus_stops)\n",
    "    texts_as_tfidf_vectors=make_tf_idf.fit_transform(preprocess_for_tfidif(text) for text in some_texts)\n",
    "    id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) ## берем ряд в нашей матрице -- он соответстует тексту\n",
    "        words_for_this_text = row_data.toarray().argsort() ## сортируем в нем все слова \n",
    "        top_words_for_this_text = words_for_this_text [0,:-1*number_of_words:-1] \n",
    "        print ([id2word[w] for w in top_words_for_this_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['скворцов', 'россия', '10', 'коронавирус', 'инфекция']\n",
      "['гайка', 'денис', 'отвинчивать', 'грузило', 'понимать']\n",
      "['стресс', 'сценарий', 'квартал', 'предприятие', 'нефть']\n",
      "['роспотребнадзор', 'лаборатория', 'тест', 'тестирование', 'коронавирус']\n",
      "['расход', 'неделя', 'россиянин', 'категория', 'товар']\n",
      "['собака', 'очумел', 'палец', 'толпа', 'знать']\n"
     ]
    }
   ],
   "source": [
    "produce_tf_idf_keywords (file_texts, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Еще один умный способ: превратить текст в граф (сеть) и искать центральные узлы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Много методов для извлечения ключевых слов основаны на сетевом анализе. Основная идея - каким-то образом перевести текст в граф, а затем посчитать центральности узлов и вывести центральные.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Mitsuru_Ishizuka/publication/2539694/figure/fig1/AS:669437975883786@1536617859438/A-word-cooccurrence-graph-of-a-set-of-news-articles-The-source-articles-are-a-set-of.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет основой графа. Ниже приведена НЕ моя реализация сборки такого графа по тексту и примения к нему алгоритма random walk для выделения ключевых слов. Вот описание от автора:\n",
    "\n",
    "Для выбора важных узлов часто используют простой randow walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### не моя реализация алгоритма\n",
    "\n",
    "def get_kws(text, top=6, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        s = np.sum(m[i])\n",
    "        if not s:\n",
    "            continue\n",
    "        m[i] /= s\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        \n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        \n",
    "        ### \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    if np.any(matrix[n]):\n",
    "        next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    else:\n",
    "        next_n = np.random.choice(range(rang))\n",
    "    return next_n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испытаем эту реализацию на наших текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'Скворцова', 'в', 'Вероника', '—']\n",
      "[',', '—', '...', 'и', '!', '?']\n",
      "[',', '.', 'и', 'на', 'в', 'нефть']\n",
      "[',', '»', 'в', '.', '«', 'и']\n",
      "[',', 'на', 'в', '%', '»', '«']\n",
      "[',', '—', 'и', '...', '!', 'в']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (get_kws (word_tokenize(text))) # функция принимает на вход "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ах да, нам ведь опять надо делать предобработку типа удаления пунктуации и стоп-слов. Давайте уже сделаем общую функцию для этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_general (input_text, stoplist):\n",
    "    '''функция для предобработки текста; \n",
    "    на вход принимает строку с текстом input_text и список стоп-слов stoplist\n",
    "    на выходе чистый список слов output'''\n",
    "    ## лемматизируем майстемом и делаем strip каждого слова:\n",
    "    output = [word.strip() for word in moi_analizator.lemmatize (input_text)] \n",
    "    ## убираем пунктуацию и стоп-слова:\n",
    "    output = [word for word in output if word not in extended_punctuation and word not in stoplist]\n",
    "    ## убираем слова, в которых вообще нет буквенных символов:\n",
    "    output = [word for word in output if re.search ('[А-ЯЁа-яёA-Za-z]', word) != None]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['федеральный', 'агентство', 'инфекция', 'бывший', 'медико-биологический', 'скворцов']\n",
      "['следователь', 'отвинчивать', 'грузило', 'понимать', 'денис', 'знать']\n",
      "['нефть', 'предприятие', 'россия', 'стресс', 'цена', 'оценивать']\n",
      "['роспотребнадзор', 'коронавирус', 'регион', 'дмитриев', 'лаборатория', 'тест']\n",
      "['неделя', 'аналитик', 'расход', 'потребление', 'категория', 'сравнение']\n",
      "['собака', 'палец', 'знать', 'бежать', 'очумел', 'ситцевый']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (get_kws (preprocessing_general(text, rus_stops)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечание: в отличие от всех прочих, этот метод недетерминированный. Поскольку в функции происходит случайное хождение по узлам, даже без смены параметров результаты будут отличаться от прогона к прогону"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вторая часть плана: тестируем на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Возьмем готовые данные, в которых уже размечены ключевые слова. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишем код для сравнения этих keywords с теми, что выделяют наши функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1. Берем реальные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные возьмем отсюда: https://github.com/mannefedov/ru_kw_eval_datasets Тут лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). В них уже размечены keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете скачать их все при помощи `git clone https://github.com/mannefedov/ru_kw_eval_datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Считываем данные "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду работать с одним файлом — ng_1.jsonlines. Как можно догадаться по названию, это файл в формате json (вы уже [познакомились с этим форматом](https://agricolamz.github.io/DS_for_DH/lists.html) в курсе Гарика). Jsonlines -- версия формата JSON, в которой хранится много json-объектов с новой строки. Поэтому целый файл не получится прочитать целиком с помощью стандартного питоновского json.load. Давайте пробовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ng_1.jsonlines\", \"r\") as read_file:\n",
    "    ng_1_data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдает ошибку парсинга json. Зато можно вот так: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_1_data = []\n",
    "with open(\"data/ng_1.jsonlines\", \"r\") as read_file:\n",
    "    for line in read_file:\n",
    "        ng_1_data.append(json.loads(line)) # json.loads считывает строку, в отличие от json.load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь в списке \"ng_1_data\" лежат объекты из нашего json. Всего 988:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (ng_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент списка соответствует тексту, у которого есть кроме самого текста заголовок, набор приписанных вручную ключевых слов, URL и краткий пересказ (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keywords': ['школа',\n",
       "  'образовательные стандарты',\n",
       "  'литература',\n",
       "  'история',\n",
       "  'фгос'],\n",
       " 'title': 'Ольга Васильева обещала \"НГ\" не перегружать школьников',\n",
       " 'url': 'https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html',\n",
       " 'content': 'В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, поверхностного разговора о них на уроке, – отмечали авторы письма. – Произведения жестко прикреплены к классам, в которых они изучаются. Однако ученики и классы всегда разные, внутренний возраст ребенка зачастую не совпадает с номером класса».\\nНакануне вчерашнего заседания Совета по ФГОСам в беседе с обозревателем «НГ» Ольга Васильева прокомментировала претензии Гильдии словесников.\\n«То число книг, которое привел господин Волков (текст письма разместил на своей страничке в Facebook учитель русского языка и литературы Сергей Волков. – «НГ») в своем открытом письме, предполагает большое количество лирических произведений, то есть стихотворений, – подчеркнула Ольга Васильева. – Вспомните, что читали вы, что читала я, что читают сейчас, – и тогда станет понятно, откуда взялась такая цифра. Там говорится не о сонме романов, а о большом количестве лирических произведений, против которого ни вы, ни я возражать не будем. Стихи – это замечательная вещь, они развивают память, душу, интеллект, вкус и далее по списку».\\nСледующий важный момент, который отметила министр в беседе с «НГ»: в процессе доработки действующих стандартов, включая литературу, не менялась общая концепция стандарта. Сохранена структура, совокупность требований к результатам реализации основных образовательных программ. А все изменения были нацелены на сохранение единого образовательного пространства.\\n«И, наконец, самое важное: новая редакция ФГОСа разработана в связи с поручением президента Российской Федерации, – заявила «НГ» Ольга Васильева. – Я процитирую его: «В целях обеспечения единого образовательного пространства на территории РФ определить в федеральных государственных образовательных стандартах начального общего, основного общего и среднего, высшего образования базовое содержание и обязательное участие основных образовательных программ, в том числе по отдельным предметам». В редакции ФГОСа, о которой мы говорим, детализованы предметные результаты отдельно для начальной школы и основной школы: по каждому предмету разработано 15–20 конкретных требований к освоению предмета».\\nОльга Васильева сравнила стандарты: в прежней редакции было 5–6 предметных результатов в предельно обобщенной форме, не конкретные. И показала разницу на примере предмета «История».\\nВ прежних стандартах результатом изучения курса истории должно было стать «развитие умений искать, сопоставлять, анализировать и оценивать содержащуюся в различных источниках информацию о событиях и явлениях, прошлых и настоящих. Способность определять и аргументировать свое отношение к ним».\\nВ новых стандартах результат развернут и конкретизирован: «Сформированность умений проводить атрибуцию текстового исторического источника (определять его авторство, время и место создания, события, явления, процессы, о которых идет речь, и др.); анализировать текст исторического источника с точки зрения его темы, цели создания, основной мысли, основной и дополнительной информации; анализировать позицию автора документа и участников событий (процессов), описываемых в историческом источнике; отвечать на вопросы по содержанию исторического источника и составлять на его основе план, таблицу, схему; соотносить содержание текстового исторического источника с другими источниками информации при изучении событий (явлений, процессов); привлекать контекстную информацию для анализа исторического источника...» И еще три подробных пункта.\\n«Кроме того, – продолжает Ольга Васильева, – в новых ФГОСах дополнены предложения по всем учебным предметам. Для каждого года обучения приведены предметные результаты обучения, обязательные элементы предметного содержания, что полностью отсутствовало в прежней редакции. Почему это важно? Я думаю, любому родителю будет интересно знать, каково содержание предмета и что изучает его ребенок в школе».\\nПосле этого она вернулась к спорам вокруг списка обязательной литературы. «Что касается «качества» списка, – заявила она, – то здесь нормативно закреплен перечень произведений, который воспринимается обществом как культурная норма… И самое важное, что, наверное, снимет большинство вопросов: мы сохраняем за учителем литературы право на собственное решение при реализации учебной программы. Что касается ажиотажа вокруг темы ФГОСа, я увязываю это с проблемой издания учебников. Проблема здесь, как мне видится, в войне, которую развязали между собой отдельные издательства».',\n",
       " 'summary': 'Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы'}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_1_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С технической точки зрения при таком считывании JSON в питон (по-умному это называется \"десериализация\") JSON-объекты превращаются в словари. По ключам можно доставать значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['школа', 'образовательные стандарты', 'литература', 'история', 'фгос']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_1_data[0] ['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['школа', 'образовательные стандарты', 'литература', 'история', 'фгос']\n",
      "['красота', 'законы']\n",
      "['юзефович', 'гражданская война', 'пепеляев', 'якутия']\n",
      "['формула1', 'автоспорт', 'гонки', 'испания', 'квят']\n"
     ]
    }
   ],
   "source": [
    "for item in ng_1_data[:4]:\n",
    "    print (item['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе стандартнымх ходом здесь было бы положить весь json внутрь pandas.DataFrame и работать с датафреймом... Но если считать, что pandas мы с вами еще не проходили, то можно обойтись и без него. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "М.б. это будет чуть менее красиво -- зато все собрано из самых простых подручных материалов (циклы + списки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Итак, применяем наши наработки по извлечению ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С самыми примитивными решениями, которые мы придумали, можно вообще за один цикл все посмотреть: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эталонные ключевые слова:  ['школа', 'образовательные стандарты', 'литература', 'история', 'фгос']\n",
      "Первые и последние слова ['в', 'среду', 'состоялось', 'отложенное', 'отдельные', 'издательства', '»', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['красота', 'законы']\n",
      "Первые и последние слова ['хорошо', ',', 'когда', 'красота', 'был', 'ее', 'укорот', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['юзефович', 'гражданская война', 'пепеляев', 'якутия']\n",
      "Первые и последние слова ['когда-то', 'леонид', 'юзефович', 'написал', 'старинными', 'местными', 'мифами', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['формула1', 'автоспорт', 'гонки', 'испания', 'квят']\n",
      "Первые и последние слова ['гран-при', 'испании', 'открыло', 'евротур', 'состоятся', '21-24', 'мая', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['есенин', 'православие', 'святая русь', 'поэзия', 'год литературы', 'клюев', 'мариенгоф', 'стихи', 'россия']\n",
      "Первые и последние слова ['десять', 'лет', 'назад', 'была', 'меня', 'к', 'себе', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['медвузы', 'медицинское образование', 'рудн', 'николай стуров', 'интервью']\n",
      "Первые и последние слова ['–', 'начну', 'с', 'главного', 'и', 'крайне', 'перспективной', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['литература', 'книги', 'периодика', 'космос', 'небо', 'астрономия', 'анатомия', 'филология']\n",
      "Первые и последние слова ['в', 'нынешнем', '«', 'бумажном', 'хватит', 'с', 'избытком', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['сша', 'ирак', 'война']\n",
      "Первые и последние слова ['едва', 'ли', 'минуло', 'полгода', 'вряд', 'ли', 'удастся', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['искусственный интеллект', 'робот', 'компьютер', 'технологии']\n",
      "Первые и последние слова ['с', 'античных', 'времен', 'философия', ',', 'подобно', 'человеку', '.']\n",
      "\n",
      "Эталонные ключевые слова:  ['вб', 'вто', 'переговоры', 'тарифы']\n",
      "Первые и последние слова ['как', 'известно', ',', 'одним', '–', 'джеймса', 'вулфенсона', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in ng_1_data[:10]:\n",
    "    print ('Эталонные ключевые слова: ', item['keywords'])\n",
    "    print ('Первые и последние слова', keywords_firstlast (item['content'], 4,4))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то не особо сходится... Попробуем частотные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эталонные ключевые слова:  ['школа', 'образовательные стандарты', 'литература', 'история', 'фгос']\n",
      "Самые частотные слова:  ['фгос', 'стандарт', 'источник', 'образовательный', 'новый', 'ольга']\n",
      "\n",
      "Эталонные ключевые слова:  ['красота', 'законы']\n",
      "Самые частотные слова:  ['красота', 'глаз', 'отчаяние', 'уходить', 'порыв', 'кошка']\n",
      "\n",
      "Эталонные ключевые слова:  ['юзефович', 'гражданская война', 'пепеляев', 'якутия']\n",
      "Самые частотные слова:  ['пепеляев', 'юзефович', 'книга', 'якутия', 'белый', 'восстание']\n",
      "\n",
      "Эталонные ключевые слова:  ['формула1', 'автоспорт', 'гонки', 'испания', 'квят']\n",
      "Самые частотные слова:  ['команда', 'гонка', 'пилот', 'ferrari', 'mclaren', 'сезон']\n",
      "\n",
      "Эталонные ключевые слова:  ['есенин', 'православие', 'святая русь', 'поэзия', 'год литературы', 'клюев', 'мариенгоф', 'стихи', 'россия']\n",
      "Самые частотные слова:  ['есенин', 'поэт', 'клюев', 'смерть', 'сергей', 'самый']\n",
      "\n",
      "Эталонные ключевые слова:  ['медвузы', 'медицинское образование', 'рудн', 'николай стуров', 'интервью']\n",
      "Самые частотные слова:  ['медицина', 'медицинский', 'кафедра', 'выпускник', 'работать', 'уровень']\n",
      "\n",
      "Эталонные ключевые слова:  ['литература', 'книги', 'периодика', 'космос', 'небо', 'астрономия', 'анатомия', 'филология']\n",
      "Самые частотные слова:  ['книга', 'русский', 'мозг', 'островной', 'говор', 'анатомия']\n",
      "\n",
      "Эталонные ключевые слова:  ['сша', 'ирак', 'война']\n",
      "Самые частотные слова:  ['ирак', 'война', 'американец', 'партизанский', 'войско', 'новый']\n",
      "\n",
      "Эталонные ключевые слова:  ['искусственный интеллект', 'робот', 'компьютер', 'технологии']\n",
      "Самые частотные слова:  ['нейросеть', 'ai', 'клетка', 'искусственный', 'интеллект', 'самый']\n",
      "\n",
      "Эталонные ключевые слова:  ['вб', 'вто', 'переговоры', 'тарифы']\n",
      "Самые частотные слова:  ['россия', 'вступление', 'кристалина', 'вто', 'георгиев', 'банк']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in ng_1_data[:10]:\n",
    "    print ('Эталонные ключевые слова: ', item['keywords'])\n",
    "    print ('Самые частотные слова: ',  keywords_most_frequent_with_stop_and_lemm (item['content'], 6, rus_stops))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что совпадения есть... \n",
    "\n",
    "Но с TF-IDF мы аналогично сделать не можем -- ведь ее расчет для одного текста требует знания о всех текстах (и поэтому наша функция produce_tf_idf_keywords принимает на вход список текстов, а не одну текстовую строку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы сначалапройдемся по считанным из json данным и заполним списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keywords = [] ## сюда запишем все ключевые слова, приписанные вручную\n",
    "full_texts = [] ## сюда тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ng_1_data:\n",
    "    manual_keywords.append(item['keywords'])\n",
    "    full_texts.append(item['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно применять к списку с текстами нашу старую функцию tf-idf. Давайте применим ее сначала к небольшому подмножеству из пары десятков текстов:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['фгос', 'стандарт', 'источник', 'ольга', 'исторический']\n",
      "['красота', 'отчаяние', 'порыв', 'кошка', 'глаз']\n",
      "['пепеляев', 'юзефович', 'якутия', 'книга', 'восстание']\n",
      "['гонка', 'команда', 'пилот', 'mclaren', 'ferrari']\n",
      "['есенин', 'поэт', 'клюев', '1925', 'борода']\n",
      "['медицина', 'медицинский', 'кафедра', 'выпускник', 'рудна']\n",
      "['мозг', 'книга', 'говор', 'островной', 'русский']\n",
      "['ирак', 'партизанский', 'война', 'американец', 'войско']\n",
      "['нейросеть', 'ai', 'клетка', 'подобно', 'интеллект']\n",
      "['кристалина', 'вступление', 'вто', 'георгиев', 'банк']\n",
      "['фильм', 'приз', 'кино', 'картина', 'медведь']\n",
      "['каша', 'рок', 'талон', 'столовая', 'сидеть']\n",
      "['псков', 'место', 'князь', 'туризм', 'город']\n",
      "['индонезия', 'терроризм', 'экстремизм', 'мусульманский', 'страна']\n",
      "['игра', 'парень', 'стример', 'саша', 'youtube']\n",
      "['выборы', 'путин', 'признавать', 'президент', 'демократический']\n",
      "['водный', 'вода', 'загрязнение', 'сток', 'очистка']\n",
      "['школа', 'образование', 'школьник', 'москва', 'олимпиада']\n",
      "['фильм', 'снимать', 'слон', 'содерберг', 'каллахан']\n",
      "['канада', 'лундквист', 'ворота', 'швед', 'олимпийский']\n"
     ]
    }
   ],
   "source": [
    "produce_tf_idf_keywords (full_texts[:20], 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит уже неплохо -- а ведь может стать и лучше, если использвать весь корпус, а не только 20 текстов.\n",
    "\n",
    "Вопрос в том, как нам оценить это \"неплохо\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества извлечения ключевых слов \n",
    "### или немного про точность, полноту и F-меру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наверно, нам надо как-то считать процент попаданий. Например, сколько слов из эталона накрыл наш алгоритм. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_tf_idf_keywords (some_texts, number_of_words):\n",
    "    result = []\n",
    "    make_tf_idf = TfidfVectorizer (stop_words=rus_stops)\n",
    "    texts_as_tfidf_vectors=make_tf_idf.fit_transform(preprocess_for_tfidif(text) for text in some_texts)\n",
    "    id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) ## берем ряд в нашей матрице -- он соответстует тексту\n",
    "        words_for_this_text = row_data.toarray().argsort() ## сортируем в нем все слова \n",
    "        top_words_for_this_text = words_for_this_text [0,:-1*number_of_words:-1] \n",
    "        result.append([id2word[w] for w in top_words_for_this_text])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим список предсказанных нами ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords = produce_tf_idf_keywords (full_texts, 6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['шувалов',\n",
       " 'рост',\n",
       " 'маневр',\n",
       " 'правительство',\n",
       " 'ндс',\n",
       " 'спираль',\n",
       " 'чудодейственный',\n",
       " 'профицит',\n",
       " 'налоговый']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keywords [125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['экономика',\n",
       " 'рост',\n",
       " 'план',\n",
       " 'правительство',\n",
       " 'налоги',\n",
       " 'игорь шувалов',\n",
       " 'инвестиции',\n",
       " 'реформы']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_keywords [125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_match_counter (list_a, list_b):\n",
    "    '''считает среднее всех пересечений (доли угаданных слов попарно для каждого текста)'''\n",
    "    all_matches = []\n",
    "    for index, words_a in enumerate (list_a):\n",
    "        words_b = list_b [index]\n",
    "        intersection = len (set(words_a) & set (words_b)) #  число элементов в пересечении списков\n",
    "        all_matches.append (intersection/len(words_a))\n",
    "    return sum(all_matches)/ len (all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06217151325957008"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Сравнимся с "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_10_per_text = produce_tf_idf_keywords (full_texts, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords_10_per_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно поменять местами списки на входе функции — и тогда мы будем искать не сколько слов в эталоне мы накрыли, а сколько из тех слов, что мы выдали, есть в эталоне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (predicted_keywords_10_per_text, manual_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут появляется возможность читерства в обратную сторону: выдавать как можно меньше слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_2_per_text = produce_tf_idf_keywords (full_texts, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2925101214574899"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_match_counter (predicted_keywords_2_per_text, manual_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
