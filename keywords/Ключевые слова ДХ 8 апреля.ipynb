{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface\n",
    "В этом практикуме многое позаимствовано у моего коллеги, преподавателя компьютерной лингвистики Миши Нефедова. Спасибо ему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое извлечение ключевых слов\n",
    "\n",
    "Извлечение ключевых слов (keyword extraction) - способ извлечения информации из текста и анализа его тематики. В отличие от автоматического реферирования (саммаризации), мы не создаем полноценный пересказ, а вытаскиваем только ключевые слова (Keywords). Иногда идут чуть дальше и извлекают ключевые словосочетания/n-граммы (Keyphrases). Например, чтобы для текста про \"морских коров\" не извлекались слова \"корова\" и \"морской\", а извлекалось все словосочетание. Это еще актуальнее для более аналитических языков вроде английского (ср. \"хотдог\" - \"hot dog\"; вряд ли вы хотите ключевое слово \"собака\" в тексте про хотдоги)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем это нужно?\n",
    "\n",
    "Задача извлечения ключевых слов обычно решается в информационном поиске. Если вы решите писать свой поисковик, или просто индесировать какую-то большую коллекцию текстов, понимать эту тему необходимо. Но и для других задач это может быть полезно. В каком-то смысле, извлечение ключевых слов — простейший способ \"тематического моделирования\" корпуса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## К делу!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План такой: \n",
    "1. Подумать, какие подходы возможны. Реализовать их на случайном примере, оценить результаты глазами\n",
    "\n",
    "1. Взять готовый набор данных, где ключевые слова уже выделены людьми, и попробовать сравнить с ними (как с эталоном) результат наших методов. Важно помнить, что понятие \"эталона\" здесь условно. Задача keyword extraction допускает альтернативные решения для одного текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ## работаем с файлами, значит, наверняка понадобится ос\n",
    "from nltk import word_tokenize ## работаем со словами — значит, токенизатор для этих файлов понадобится\n",
    "import string ## возьмем оттуда punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самый тупой вариант: берем первые и последние слова текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте возьмем для эксперимента 10 текстов: по 5 новостных и художественных. Скачать можно тут. У меня они лежат в папке 'samples_for_class' рядом с кодом. Положу этот путь в переменную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEXTS = 'samples_for_class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем фукнцию, которая берет первые и последние слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_firstlast (some_text, num_first, num_last):\n",
    "    tokenized_text = [word for word in word_tokenize (some_text) if word not in string.punctuation]\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним по текстам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts = [] ## в этот пустой список я запишу тексты файлов, чтобы потом пименять к ним разные методы\n",
    "for some_file in os.listdir (PATH_TO_TEXTS): # в этом цикле я сложу в file_texts тексты файлов, лежащие по адресу в PATH_TO_TEXTS\n",
    "    if some_file.endswith ('.txt'):\n",
    "        with open (os.path.join(PATH_TO_TEXTS, some_file),'r') as open_file:\n",
    "            file_texts.append (open_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast (text, 3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй самый тупой вариант: берем самые частотные слова текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist # как вы помните, в нлтк есть счетчик частотностей\n",
    "# но можно и пользоваться Counter из модуля collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_most_frequent (some_text, num_most_freq):\n",
    "    tokenized_text = [word for word in word_tokenize (some_text) if word not in string.punctuation+'—»«...']\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent (text, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Третий вариант: берем самые частотные слова текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "rus_stops = set(stopwords.words('russian')) ## русские стоп-слова из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop (some_text, num_most_freq, stoplist):\n",
    "    tokenized_text = [word for word in word_tokenize (some_text) if word not in string.punctuation+'—»«...' and word not in stoplist]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop (text, 6, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Четвертый вариант: берем самые частотные ЛЕММЫ текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem ## используем mystem в обертке pymystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moi_analizator = Mystem() ## создаем экземпляр класса \"анализатор MyStem\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passed_filter (some_word, stoplist):\n",
    "    if some_word in string.punctuation +'—»«... ':\n",
    "        return False\n",
    "    elif re.search (re.compile('['+string.punctuation+'—»«... \\n'+']'), some_word) != None:\n",
    "        return False\n",
    "    elif some_word in stoplist:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop_and_lemm (some_text, num_most_freq, stoplist):\n",
    "    lemmatized_text = [word for word in moi_analizator.lemmatize(some_text.lower()) if passed_filter(word, stoplist)]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(lemmatized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop_and_lemm (text, 6, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А какие-нибудь более умные варианты будут?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Meet TF-IDF ! \n",
    "\n",
    "TF IDF — это мера, которая учитывает не только частотность слова в документе (TF, term frequency) — но и то, насколько часто — вернее, насколько редко! — оно встречается во всем корпусе (IDF, inverse document frequency). Формально она считается так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/proxy/wvgUX_ST82yijsMIG5Sq05e_rY6OrwcUC9wvG4VvElAwdAGUb3oxeofaXucPkZYbyCyW-A6mW2Tboi8OrU96-KR4YSIx6jSDDv12CwRBq26EavdKsXVj\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть в том, что таким образом повышаются слова, частотные в данном документе -- и редкие в корпусе в целом. Такие слова получают бонус относительно слов, частотных и в данном документе, и в других. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer ## готовая реализация TF IDF из библиотеки sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tf_idf = TfidfVectorizer (stop_words=rus_stops) # Создаем специальный объект-векторайзер \n",
    "#fitted_vectorizer = make_tf_idf.fit(file_texts) \n",
    "texts_as_tfidf_vectors=make_tf_idf.fit_transform(file_texts) # Кладем в этот векторайзер наши файлы и просим сделать матрицу TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (texts_vectors.shape) ## посмотрим размерность матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tf_idf.get_feature_names() ## посмотрим "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## кусочек кода, который берет матрицу TF-IDF и выдает по ней топ-слова для каждого текста\n",
    "\n",
    "id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "for text_row in range(texts_vectors.shape[0]): \n",
    "    row_data = texts_vectors.getrow(text_row) ## берем ряд в нашей матрице -- он соответстует етксты\n",
    "    words_for_this_text = row_data.toarray().argsort() ## сортируем в нем все слова \n",
    "    top_words_for_this_text = words_for_this_text [0,:-6:-1] \n",
    "    print ([id2word[w] for w in top_words_for_this_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Естественно, тут можно снова накрутить лемматизацию и стоп-слова получше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Еще один умный способ: превратить текст в граф (сеть) и искать центральные узлы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Много методов для извлечения ключевых слов основаны на сетевом анализе. Основная идея - каким-то образом перевести текст в граф, а затем посчитать центральности узлов и вывести центральные.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Mitsuru_Ishizuka/publication/2539694/figure/fig1/AS:669437975883786@1536617859438/A-word-cooccurrence-graph-of-a-set-of-news-articles-The-source-articles-are-a-set-of.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет основой графа. Ниже приведена НЕ моя реализация сборки такого графа по тексту и примения к нему алгоритма random walk для выделения ключевых слов. Вот описание от автора:\n",
    "\n",
    "Для выбора важных узлов часто используют простой randow walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### не моя реализация алгоритма\n",
    "\n",
    "def get_kws(text, top=6, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        s = np.sum(m[i])\n",
    "        if not s:\n",
    "            continue\n",
    "        m[i] /= s\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        \n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        \n",
    "        ### \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    if np.any(matrix[n]):\n",
    "        next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    else:\n",
    "        next_n = np.random.choice(range(rang))\n",
    "    return next_n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испытаем эту реализацию на наших текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['россия', 'скворцов', 'инфекция', '10', 'коронавирус', 'коронавирусный']\n",
      "['денис', 'гайка', 'отвинчивать', 'грузило', 'это', 'понимать']\n",
      "['год', 'стресс', 'сценарий', 'квартал', 'нефть', 'предприятие']\n",
      "['тестирование', 'лаборатория', 'тест', 'роспотребнадзор', 'коронавирус', 'биоматериал']\n",
      "['неделя', 'расход', 'россиянин', 'категория', 'товар', '9']\n",
      "['собака', 'это', 'очумел', 'палец', 'знать', 'толпа']\n"
     ]
    }
   ],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop_and_lemm (text, 6, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В следующий раз — вторая часть плана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). В них уже размечены keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишем код для сравнения этих keywords с теми, что выделяют наши функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
