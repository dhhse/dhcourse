{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface\n",
    "В этом практикуме многое позаимствовано у моего коллеги, преподавателя компьютерной лингвистики Миши Нефедова. Спасибо ему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое извлечение ключевых слов\n",
    "\n",
    "Извлечение ключевых слов (keyword extraction) - способ извлечения информации из текста и анализа его тематики. В отличие от автоматического реферирования (саммаризации), мы не создаем полноценный пересказ, а вытаскиваем только ключевые слова (Keywords). Иногда идут чуть дальше и извлекают ключевые словосочетания/n-граммы (Keyphrases). Например, чтобы для текста про \"морских коров\" не извлекались слова \"корова\" и \"морской\", а извлекалось все словосочетание. Это еще актуальнее для более аналитических языков вроде английского (ср. \"хотдог\" - \"hot dog\"; вряд ли вы хотите ключевое слово \"собака\" в тексте про хотдоги)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем это нужно?\n",
    "\n",
    "Задача извлечения ключевых слов обычно решается в информационном поиске. Если вы решите писать свой поисковик, или просто индесировать какую-то большую коллекцию текстов, понимать эту тему необходимо. Но и для других задач это может быть полезно. В каком-то смысле, извлечение ключевых слов — простейший способ \"тематического моделирования\" корпуса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## К делу!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План такой: \n",
    "1. Подумать, какие подходы возможны. Реализовать их на случайном примере, оценить результаты глазами\n",
    "\n",
    "1. Взять готовый набор данных, где ключевые слова уже выделены людьми, и попробовать сравнить с ними (как с эталоном) результат наших методов. Важно помнить, что понятие \"эталона\" здесь условно. Задача keyword extraction допускает альтернативные решения для одного текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os ## работаем с файлами, значит, наверняка понадобится ос\n",
    "from nltk import word_tokenize ## работаем со словами — значит, токенизатор для этих файлов понадобится\n",
    "import string ## возьмем оттуда punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте возьмем для эксперимента 5 текстов: \n",
    "* новость РБК про сокращение трат россиян\n",
    "* новость Медузы про тестирование на коронавирус\n",
    "* сказка \"Колобок\"\n",
    "* повесть Пушкина \"Капитанская дочка\" \n",
    "* рассказ Чехова \"Злоумышленник\". \n",
    "\n",
    "Скачать можно [тут](https://github.com/dhhse/dhcourse/tree/gh-pages/keywords/samples_13_04.zip). У меня они лежат в папке 'samples_13_04' рядом с кодом. Положу этот путь в переменную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_TEXTS = 'samples_13_04'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запишу тексты всех файлов в один список, чтобы потом с ними легче работать: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## в этот пустой список я потом запишу тексты файлов, чтобы потом применять к ним разные методы:\n",
    "file_texts = []\n",
    "# в этом цикле я сложу в file_texts тексты файлов, лежащие по адресу в PATH_TO_TEXTS:\n",
    "for some_file in os.listdir (PATH_TO_TEXTS): \n",
    "    if some_file.endswith ('.txt'):\n",
    "        with open (os.path.join(PATH_TO_TEXTS, some_file),'r') as open_file:\n",
    "            file_texts.append (open_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пробуем подходы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самый тупой вариант: берем первые и последние слова текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем фукнцию, которая берет первые и последние слова (их число задается пользователем как параметр функции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    ## разобьем текст на токены\n",
    "    tokenized_text = word_tokenize (some_text.lower())\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним по текстам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast (text, 6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось не очень. А еще мешают знаки препинания... Давайте их отфильтруем (на этапе заполнения переменной tokenized_text). При добавлении в tokenized_text будем проверять, что помещаемый в список элемент не является пунктуатором). Сначала напишем это в виде цикла с if-ом внутри:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = []\n",
    "    # пройдемся циклом по результату токенизации текста\n",
    "    for word in word_tokenize(some_text.lower()):\n",
    "        # выцепим все слова, которые не входят в string.punctuation\n",
    "        if word not in string.punctuation:\n",
    "            # сложим их в переменую tokenized_text, с которой мы работаем дальше\n",
    "            tokenized_text.append (word)\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast_no_punct (text, 6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь сделаем этот код компактнее, использовав генератор списка, о которых как раз говорил вам Борис Валерьевич на последем занятии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = [word for word in word_tokenize(some_text.lower()) if word not in string.punctuation]\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast_no_punct (text, 6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше, но в стандартную сборку string.punctuation явно входит не все, что нам нужно. Усовершенствуем ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_punctuation = string.punctuation + '—»«...' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем функцию, добавив туда extended_punctuation вместо string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_firstlast_no_punct (some_text, num_first, num_last):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число слов от начала num_first, число слов от конца num_last\"\"\"\n",
    "    tokenized_text = [word for word in word_tokenize(some_text.lower()) if word not in extended_punctuation]\n",
    "    if len (tokenized_text) > num_first + num_last:\n",
    "        if num_last != 0: \n",
    "            return tokenized_text [:num_first] + tokenized_text [num_last*-1:]\n",
    "        else: \n",
    "            return tokenized_text [:num_first] # спецобработка случая с нулем слов с конца\n",
    "    else:\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_firstlast_no_punct (text, 6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Менее тупой вариант: берем самые частотные слова текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist # как вы помните, в нлтк есть счетчик частотностей\n",
    "# но можно и пользоваться Counter из модуля collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краткое напоминание: как работает FreqDist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'повар петр повар павел петр пек'\n",
    "FreqDist(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(text.split()).most_common (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent (some_text, num_most_freq):\n",
    "    \"\"\"На вход -- строка с текстом some_text, число самых частотных слов от начала num_most_freq\"\"\"\n",
    "    # запишем в переменную tokenized_text список токенов без пунктуации\n",
    "    tokenized_text = [word for word in word_tokenize (some_text.lower()) if word not in extended_punctuation] \n",
    "    # вернем самые частотные слова, посчитанные с помощью метода most_common у FreqDist \n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent (text, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Третий вариант: берем самые частотные слова текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open ('stop_ru.txt', 'r') as stop_file:\n",
    "    rus_stops = [word.strip() for word in stop_file.readlines()] # запишем стослова в список "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop (some_text, num_most_freq, some_stoplist):\n",
    "    tokenized_text = [word for word in word_tokenize (some_text.lower()) if word not in extended_punctuation and word not in some_stoplist]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(tokenized_text).most_common(num_most_freq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop (text, 6, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Четвертый вариант: берем самые частотные ЛЕММЫ текста без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem ## используем mystem в обертке pymystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moi_analizator = Mystem() ## создаем экземпляр класса \"анализатор MyStem\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop_and_lemm (some_text, num_most_freq, some_stoplist):\n",
    "    lemmatized_text = [word for word in moi_analizator.lemmatize(some_text.lower()) if word.strip() not in extended_punctuation and word not in some_stoplist]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(lemmatized_text).most_common(num_most_freq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop_and_lemm (text, 10, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Майстем наловил нам всякой пунктуации. Нужны еще фильтры. А вторая строчка функции разрослась и не читается. Давайте вынесем все эти фильтры в отдельную функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def passed_filter (some_word, stoplist):\n",
    "    some_word = some_word.strip()\n",
    "    if some_word in extended_punctuation:\n",
    "        return False\n",
    "    elif some_word in stoplist:\n",
    "        return False\n",
    "    elif re.search ('[А-ЯЁа-яёA-Za-z]', some_word) == None:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def keywords_most_frequent_with_stop_and_lemm (some_text, num_most_freq, stoplist):\n",
    "    lemmatized_text = [word for word in moi_analizator.lemmatize(some_text.lower()) if passed_filter(word, stoplist)]\n",
    "    return [word_freq_pair[0] for word_freq_pair in FreqDist(lemmatized_text).most_common(num_most_freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (keywords_most_frequent_with_stop_and_lemm (text, 10, rus_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А какие-нибудь более продвинутые варианты будут?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Meet TF-IDF ! \n",
    "\n",
    "TF IDF — это мера, которая учитывает не только частотность слова в документе (TF, term frequency) — но и то, насколько часто — вернее, насколько редко! — оно встречается во всем корпусе (IDF, inverse document frequency). Формально она считается так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/tfidf.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть в том, что таким образом повышаются слова, частотные в данном документе -- и редкие в корпусе в целом. Такие слова получают бонус относительно слов, частотных и в данном документе, и в других. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer ## готовая реализация TF IDF из библиотеки sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_tf_idf = TfidfVectorizer (stop_words=rus_stops) # Создаем специальный объект-векторайзер \n",
    "#fitted_vectorizer = make_tf_idf.fit(file_texts) \n",
    "texts_as_tfidf_vectors=make_tf_idf.fit_transform(file_texts) # Кладем в этот векторайзер наши файлы и просим сделать матрицу TF_IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_as_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (texts_as_tfidf_vectors.shape) ## посмотрим размерность матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_tf_idf.get_feature_names()[100:200] ## посмотрим "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## кусочек кода, который берет матрицу TF-IDF и выдает по ней топ-слова для каждого текста\n",
    "\n",
    "## в словарь id2word запишем соответствия между числовами индексами слов, которые хранятся \n",
    "## в матрице tfidf -- и самими словами:\n",
    "id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "\n",
    "# теперь пройдемся по матрице и вытащим для каждого текста слова с самым большим tfidf\n",
    "\n",
    "for text_row in range(texts_as_tfidf_vectors.shape[0]):\n",
    "    ## берем ряд в нашей матрице -- он соответстует тексту\n",
    "    row_data = texts_as_tfidf_vectors.getrow(text_row) \n",
    "    ## сортируем в нем все слова (вернее, индексы слов) -- получаем от самых маленьких к самым большим\n",
    "    words_for_this_text = row_data.toarray().argsort() \n",
    "    ## берем крайние 6 слов отсортированного ряда\n",
    "    top_words_for_this_text = words_for_this_text [0,:-6:-1] \n",
    "    print ([id2word[w] for w in top_words_for_this_text]) ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Естественно, тут можно снова накрутить лемматизацию "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пусть текст лемматизируется в отдельной функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_tfidif (some_text):\n",
    "    lemmatized_text = moi_analizator.lemmatize(some_text.lower())\n",
    "    return (' '.join(lemmatized_text)) # поскольку tfidf векторайзер принимает на вход строку, \n",
    "    #после лемматизации склеим все обратно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам код для расчета tfidf на корпусе текстов давайте тоже упакуем в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_tf_idf_keywords (some_texts, number_of_words):\n",
    "    make_tf_idf = TfidfVectorizer (stop_words=rus_stops)\n",
    "    texts_as_tfidf_vectors=make_tf_idf.fit_transform(preprocess_for_tfidif(text) for text in some_texts)\n",
    "    id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "\n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        ## берем ряд в нашей матрице -- он соответстует тексту:\n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row)\n",
    "        ## сортируем в нем все слова: \n",
    "        words_for_this_text = row_data.toarray().argsort() \n",
    "        ## берем число слов с конца, равное number_of_words \n",
    "        top_words_for_this_text = words_for_this_text [0, :-1*(number_of_words+1):-1]\n",
    "        ## печатаем результат\n",
    "        print([id2word[w] for w in top_words_for_this_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_tf_idf_keywords (file_texts, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Еще один умный способ: превратить текст в граф (сеть) и искать центральные узлы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Много методов для извлечения ключевых слов основаны на сетевом анализе. Основная идея - каким-то образом перевести текст в граф, а затем посчитать центральности узлов и вывести центральные.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Mitsuru_Ishizuka/publication/2539694/figure/fig1/AS:669437975883786@1536617859438/A-word-cooccurrence-graph-of-a-set-of-news-articles-The-source-articles-are-a-set-of.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто применяют такой подход - построим матрицу совстречаемости слов (в каком-то окне), эта матрица будет основой графа. Ниже приведена НЕ моя реализация сборки такого графа по тексту и примения к нему алгоритма random walk для выделения ключевых слов. Вот описание от автора:\n",
    "\n",
    "Для выбора важных узлов часто используют простой random walk. Алгоритм примерно такой:  \n",
    "1) Каким-то образом выбирается первый узел графа (например, случайно из равномерного распределения)  \n",
    "2) на основе связей этого узла с другими, выбирается следующий узел  \n",
    "3) шаг два повторяется некоторое количество раз (например, тысячу) __*чтобы не зацикливаться, с какой-то вероятностью мы случайно перескакиваем на другой узел (даже если он никак не связан с текущим, как в шаге 1)__  \n",
    "5) на каждом шаге мы сохраняем узел в котором находимся  \n",
    "6) в конце мы считаем в каких узлах мы были чаще всего и выводим top-N  \n",
    "\n",
    "\n",
    "Предполагается, что мы часто будем приходить в важные узлы графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### не моя реализация алгоритма\n",
    "\n",
    "def get_kws(text, top=6, window_size=5, random_p=0.1):\n",
    "\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    # нормализуем строки, чтобы получилась вероятность перехода\n",
    "    for i in range(m.shape[0]):\n",
    "        s = np.sum(m[i])\n",
    "        if not s:\n",
    "            continue\n",
    "        m[i] /= s\n",
    "    \n",
    "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
    "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
    "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
    "    \n",
    "    c = Counter()\n",
    "    # начнем с абсолютного случайно выбранного элемента\n",
    "    n = np.random.choice(len(vocab))\n",
    "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
    "        \n",
    "        # c вероятностью random_p \n",
    "        # перескакиваем на другой узел\n",
    "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
    "        \n",
    "        if go_random:\n",
    "            n = np.random.choice(len(vocab))\n",
    "        \n",
    "        \n",
    "        ### \n",
    "        n = take_step(n, m)\n",
    "        # записываем узлы, в которых были\n",
    "        c.update([n])\n",
    "    \n",
    "    # вернем топ-N наиболее часто встретившихся сл\n",
    "    return [id2word[i] for i, count in c.most_common(top)]\n",
    "\n",
    "def take_step(n, matrix):\n",
    "    rang = len(matrix[n])\n",
    "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
    "    if np.any(matrix[n]):\n",
    "        next_n = np.random.choice(range(rang), p=matrix[n])\n",
    "    else:\n",
    "        next_n = np.random.choice(range(rang))\n",
    "    return next_n\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испытаем эту реализацию на наших текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (get_kws (word_tokenize(text))) # функция принимает на вход список слов, а не строку, поэтому так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ах да, нам ведь опять надо делать предобработку типа удаления пунктуации и стоп-слов. Давайте уже сделаем общую функцию для этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_general (input_text, stoplist):\n",
    "    '''функция для предобработки текста; \n",
    "    на вход принимает строку с текстом input_text и список стоп-слов stoplist\n",
    "    на выходе чистый список слов output'''\n",
    "    ## лемматизируем майстемом и делаем strip каждого слова:\n",
    "    output = [word.strip() for word in moi_analizator.lemmatize (input_text)] \n",
    "    ## убираем пунктуацию и стоп-слова:\n",
    "    output = [word for word in output if word not in extended_punctuation and word not in stoplist]\n",
    "    ## убираем слова, в которых вообще нет буквенных символов:\n",
    "    output = [word for word in output if re.search ('[А-ЯЁа-яёA-Za-z]', word) != None]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in file_texts:\n",
    "    print (get_kws (preprocessing_general(text, rus_stops)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечание: в отличие от всех прочих, этот метод недетерминированный. Поскольку в функции происходит случайное хождение по узлам, даже без смены параметров результаты будут отличаться от прогона к прогону"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вторая часть плана: тестируем на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Возьмем готовые данные, в которых уже размечены ключевые слова. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишем код для сравнения этих keywords с теми, что выделяют наши функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1. Берем реальные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные возьмем отсюда: https://github.com/mannefedov/ru_kw_eval_datasets Тут лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). В них уже размечены keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете скачать их все при помощи `git clone https://github.com/mannefedov/ru_kw_eval_datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Считываем данные "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду работать с одним файлом — ng_1.jsonlines. Как можно догадаться по названию, это файл в формате json (вы уже [познакомились с этим форматом](https://agricolamz.github.io/DS_for_DH/lists.html) в курсе Гарика). Jsonlines -- версия формата JSON, в которой хранится много json-объектов с новой строки. Поэтому целый файл не получится прочитать целиком с помощью стандартного питоновского json.load. Давайте пробовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ng_1.jsonlines\", \"r\") as read_file:\n",
    "    ng_1_data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдает ошибку парсинга json. Зато можно вот так: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_1_data = []\n",
    "with open(\"data/ng_1.jsonlines\", \"r\") as read_file:\n",
    "    for line in read_file:\n",
    "        ng_1_data.append(json.loads(line)) # json.loads считывает строку, в отличие от json.load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь в списке \"ng_1_data\" лежат объекты из нашего json. Всего 988:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (ng_1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент списка соответствует тексту, у которого есть кроме самого текста заголовок, набор приписанных вручную ключевых слов, URL и краткий пересказ (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_1_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С технической точки зрения при таком считывании JSON в питон (по-умному это называется \"десериализация\") JSON-объекты превращаются в словари. По ключам можно доставать значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_1_data[0] ['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ng_1_data[:4]:\n",
    "    print (item['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе стандартнымх ходом здесь было бы положить весь json внутрь pandas.DataFrame и работать с датафреймом... Но если считать, что pandas мы с вами еще не проходили, то можно обойтись и без него. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "М.б. это будет чуть менее красиво -- зато все собрано из самых простых подручных материалов (циклы + списки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Итак, наконец-то применяем наши наработки по извлечению ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С самыми примитивными решениями, которые мы придумали, можно вообще за один цикл все посмотреть: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ng_1_data[:10]:\n",
    "    print ('Эталонные ключевые слова: ', item['keywords'])\n",
    "    print ('Первые и последние слова', keywords_firstlast (item['content'], 4,4))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то не особо сходится... Попробуем частотные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ng_1_data[:10]:\n",
    "    print ('Эталонные ключевые слова: ', item['keywords'])\n",
    "    print ('Самые частотные слова: ',  keywords_most_frequent_with_stop_and_lemm (item['content'], 6, rus_stops))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что совпадения есть... \n",
    "\n",
    "Но с TF-IDF мы аналогично сделать не можем -- ведь ее расчет для одного текста требует знания о всех текстах (и поэтому наша функция produce_tf_idf_keywords принимает на вход список текстов, а не одну текстовую строку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы сначалапройдемся по считанным из json данным и заполним списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keywords = [] ## сюда запишем все ключевые слова, приписанные вручную\n",
    "full_texts = [] ## сюда тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ng_1_data:\n",
    "    manual_keywords.append(item['keywords'])\n",
    "    full_texts.append(item['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно применять к списку с текстами нашу старую функцию tf-idf. Давайте применим ее сначала к небольшому подмножеству из пары десятков текстов:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_tf_idf_keywords (full_texts[:20], 6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keywords [:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит уже неплохо -- а ведь может стать и лучше, если использвать весь корпус, а не только 20 текстов.\n",
    "\n",
    "Вопрос в том, как нам оценить это \"неплохо\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества извлечения ключевых слов \n",
    "### или немного про точность и полноту "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наверно, нам надо как-то считать процент попаданий. Например, сколько слов из эталона накрыл наш алгоритм. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала создадим список предсказанных нами ключевых слов. Для этого придется немножко видоизменить функцию для tfidf -- ведь она раньше просто печатала  keywords для каждого текста -- а нам надо, чтобы она вернула последовательный список keywords для всех текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_tf_idf_keywords (some_texts, number_of_words):\n",
    "    result = []\n",
    "    make_tf_idf = TfidfVectorizer (stop_words=rus_stops)\n",
    "    texts_as_tfidf_vectors=make_tf_idf.fit_transform(preprocess_for_tfidif(text) for text in some_texts)\n",
    "    id2word = {i:word for i,word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) ## берем ряд в нашей матрице -- он соответстует тексту\n",
    "        words_for_this_text = row_data.toarray().argsort() ## сортируем в нем все слова \n",
    "        top_words_for_this_text = words_for_this_text [0,:-1*number_of_words:-1] \n",
    "        result.append([id2word[w] for w in top_words_for_this_text])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords = produce_tf_idf_keywords (full_texts, 6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_keywords [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(20):\n",
    "    print (manual_keywords [index])\n",
    "    print (predicted_keywords[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_match_counter (list_a, list_b):\n",
    "    '''считает среднее всех пересечений (доли угаданных нами ключ.слов для каждого текста)'''\n",
    "    all_matches = []\n",
    "    for index, words_a in enumerate (list_a):\n",
    "        words_b = list_b [index]\n",
    "        intersection = len (set(words_a) & set (words_b)) #  число элементов в пересечении списков\n",
    "        all_matches.append (intersection/len(words_a))\n",
    "    return sum(all_matches)/ len (all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнимся с простой частотностью слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_firstlast = [keywords_firstlast (text, 6, 6) for text in full_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords_firstlast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_firstlast_no_punct = [keywords_firstlast_no_punct (text, 6, 6) for text in full_texts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords_firstlast_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_freq_lemm = [keywords_most_frequent_with_stop_and_lemm(text, 6, rus_stops) for text in full_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords_freq_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема такого подхода — легко повысить результат, просто увеличив число выдаваемых слов для одного текста. Ведь тогда мы наверняка накроем больше верных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_10_per_text = produce_tf_idf_keywords (full_texts, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (manual_keywords, predicted_keywords_10_per_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно поменять местами списки на входе функции — и тогда мы будем искать не сколько слов в эталоне мы накрыли, а сколько из тех слов, что мы выдали, есть в эталоне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (predicted_keywords_10_per_text, manual_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут появляется возможность читерства в обратную сторону: выдавать как можно меньше слов для одного текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keywords_2_per_text = produce_tf_idf_keywords (full_texts, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_match_counter (predicted_keywords_2_per_text, manual_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность, полнота и F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Полнота\n",
    "Когда мы вначале смотрели, какая часть слов в эталоне накрыта нашей predicted-выдачей, мы, в сущности, считали __полноту__ (recall) нашего алгоритма. То есть какую долю всех верных ответов мы находим (не принимая во внимание свои \"неверные\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Точность\n",
    "Когда мы поменяли местами predicted и manual, функция стала считать, какая часть слов в выдаче есть в эталоне, мы, в сущности, считали __точность__ (precision) нашего алгоритма. То есть какую долю среди наших ответов составляют верные ответы (не принимая во внимание верные ответы эталона, которые мы не выдали). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-мера\n",
    "\n",
    "Я попытался показать, что и ту, и другую метрику можно хакнуть — они слишком однобоки. Ученые это давно поняли и придумали усредняющую метрику — F1-меру. Это \"гармоническое среднее\" точности и полноты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pics/fmeasure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немножко доработаем нашу функцию simple_match_counter , чтобы она считала сразу точность, полноту и F-меру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_fmeasure (manual, predicted):\n",
    "    '''считает точность, полноту и F-меру'''\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for index, words_manual in enumerate (manual):\n",
    "        words_predicted = predicted [index]\n",
    "        intersection = len (set(words_manual) & set (words_predicted)) #  число элементов в пересечении списков\n",
    "        recalls.append (intersection/len(words_manual)) \n",
    "        precisions.append (intersection/len(words_predicted))\n",
    "        \n",
    "    mean_precision = sum(precisions)/ len (precisions)\n",
    "    mean_recall = sum(recalls)/ len (recalls)\n",
    "    fmeasure =  ((2*mean_recall*mean_precision)/(mean_recall+mean_precision))\n",
    "    return 'Точность: {}, полнота: {}, F-мера {}'.format (mean_precision, mean_recall, fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fmeasure (manual_keywords, predicted_keywords_freq_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание по ключевым словам: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите сами любой алгоритм извлечения ключевых слов. Какой угодно, пусть даже интуитивно он будет нелепый, неважно. Конечно, будет интересно придумать что-то хитрое, с опорой на лингвистику (пока ее тут почти что не было). Или свой графовый алгоритм -- можно с networkx, а не на матрицах:) Еще круче, если вы сделаете не только слова, но и фразы (чтобы был шанс выловить \"образовательные стандарты\" или \"леонида юзефовича\" целиком). Но можно и воспроизвести что-то из того, что есть выше. Но только не копируйте код, а пишите сами, воспроизводя логику. \n",
    "\n",
    "2. Возьмите любой собственный набор текстов. Примените алгоритм к текстам, выведите результаты на экран.\n",
    "\n",
    "2. Протестируйте алгоритм на любом документе (документах) из датасета https://github.com/mannefedov/ru_kw_eval_datasets Будет хорошо, если вы напишете свою функцию измерения точности, полноты и F-меры. Желающие могут добавить туде еще расчет [кэффициента близости Жаккара](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%96%D0%B0%D0%BA%D0%BA%D0%B0%D1%80%D0%B0). \n",
    "\n",
    "3. Сравните результат с тем, который получается, если просто брать все частотные леммы.\n",
    "\n",
    "4. Результат в любом виде (.py, .ipynb) выкладывайте на свой гитхаб.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
