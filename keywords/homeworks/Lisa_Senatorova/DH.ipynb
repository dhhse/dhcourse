{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "from pymystem3 import Mystem as mystem\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer as rt\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('ru')\n",
    "from nltk import FreqDist\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем список текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_texts = []\n",
    "for some_file in os.listdir ('./Corpus'): \n",
    "    if some_file.endswith ('.txt'):\n",
    "        with open (os.path.join('./Corpus', some_file),'r') as open_file:\n",
    "            list_of_texts.append(open_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем функцию для лемматизации одного текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_of_one_text(text):\n",
    "    mystem = Mystem()\n",
    "    lemmas = mystem.lemmatize(text)\n",
    "    for word_idx in range(len(lemmas)):\n",
    "        lemmas[word_idx] = lemmas[word_idx].strip('\\n').strip('\\xa0 ')\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    clean_lemmas = re.sub('\\d+?', '', lemmas)\n",
    "    return clean_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим функцию, которая пролемматизирует нам все тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmas_of_all_texts(list_of_texts):\n",
    "    lemmas_of_all_texts = []\n",
    "    for i in range(len(list_of_texts)):\n",
    "        lemmas_of_all_texts.append(lemmatization(list_of_texts[i]))\n",
    "    return lemmas_of_all_texts   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lemmas = lemmas_of_all_texts(list_of_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## То же самое создадим для токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tokenizer = rt('\\w+')\n",
    "    tokenized_lemmas = tokenizer.tokenize(text)\n",
    "    return tokenized_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_of_all_texts(list_of_lemmas):\n",
    "    tokens_of_all_texts = []\n",
    "    for i in range(len(list_of_texts)):\n",
    "        tokens_of_all_texts.append(tokenization(list_of_lemmas[i]))\n",
    "    return tokens_of_all_texts   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tokenized_texts = tokenization_of_all_texts(list_of_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь почистим от стоп-слов (в какой-то степени это опционально, особенно, если мы работаем с биграмами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop_words_from_text_list(list_of_tokenized_texts):\n",
    "    for i in range(len(list_of_tokenized_texts)):\n",
    "        for stop_word in stop_words:\n",
    "            list_of_tokenized_texts[i] = list(filter(lambda word: word != stop_word, list_of_tokenized_texts[i]))\n",
    "    return list_of_tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = drop_stop_words_from_text_list(list_of_tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoshenko = no_stop_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "QGIS = no_stop_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = no_stop_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gus = no_stop_words[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем создать пары слов для каждого нашего текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def couple(list_of_tokens):\n",
    "    couples = []\n",
    "    for i, word in enumerate(list_of_tokens[:-1]):\n",
    "        couple = word + ' '+ list_of_tokens[i+1]\n",
    "        couples.append(couple)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoshenko_couples = couple(list_of_tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "QGIS_couples  = couple(list_of_tokenized_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1_couples  = couple(list_of_tokenized_texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gus_couples  = couple(list_of_tokenized_texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь создадим словари частотности. Первый подходит просто для подсчета слов, в то время как второй нужен для биграмов, причем в нем дуплеты типа \"он пошел\" и \"пошел он\" слиты воедино"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_words(list_of_words):\n",
    "    dict_with_quantity = {}\n",
    "    for word in list_of_words:\n",
    "        if word in dict_with_quantity:\n",
    "            dict_with_quantity[word] +=1\n",
    "        else:\n",
    "            dict_with_quantity[word]=1\n",
    "    return dict_with_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_similar_pair_values(pair_dic):\n",
    "    filter_pair_dic = {}\n",
    "    for key1 in pair_dic.keys():\n",
    "        for key2 in pair_dic.keys():\n",
    "            if key1[0] == key2[1] and key1[1] == key2[0] and key1[0] < key1[1]:\n",
    "                filter_pair_dic[key1] = pair_dic[key1] + pair_dic[key2]\n",
    "    return filter_pair_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(final_dictionary):\n",
    "    itog = dict(sorted(final_dictionary.items(), key=lambda x: x[1], reverse=True))\n",
    "    return itog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_slice(dictionary, index):\n",
    "    sliced_dictionary = dict(itertools.islice(sort(counting_words(dictionary)).items(), index))\n",
    "    return sliced_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_couples_slice(dictionary, index):\n",
    "    sliced_dictionary = dict(itertools.islice(sort(sum_similar_pair_values(counting_words(dictionary))).items(), index))\n",
    "    return sliced_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итак, время посмотреть на частотности!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частотности для одиноких слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'говорить': 24,\n",
       " 'выходить': 9,\n",
       " 'пьеса': 9,\n",
       " 'гражданин': 7,\n",
       " 'публика': 6,\n",
       " 'купец': 6,\n",
       " 'кричать': 6,\n",
       " 'любитель': 5,\n",
       " 'играть': 4,\n",
       " 'дескать': 4}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_slice(zoshenko, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'экзопланета': 11,\n",
       " 'проксим': 8,\n",
       " 'звезда': 6,\n",
       " 'обнаруживать': 6,\n",
       " 'астроном': 5,\n",
       " 'наблюдение': 5,\n",
       " 'объект': 4,\n",
       " 'кандидат': 4,\n",
       " 'помощь': 4,\n",
       " 'телескоп': 3}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_slice(N_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'карта': 15,\n",
       " 'данные': 10,\n",
       " 'гис': 9,\n",
       " 'проекция': 7,\n",
       " 'система': 6,\n",
       " 'работа': 5,\n",
       " 'например': 5,\n",
       " 'изображение': 5,\n",
       " 'QGIS': 4,\n",
       " 'использоваться': 4}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_slice(QGIS, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'гусь': 64,\n",
       " 'птица': 10,\n",
       " 'конец': 7,\n",
       " 'роспись': 7,\n",
       " 'гусыня': 7,\n",
       " 'город': 7,\n",
       " 'римлянин': 6,\n",
       " 'лебедь': 6,\n",
       " 'сократ': 6,\n",
       " 'матушка': 6}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_slice(Gus, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частотности для пар слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'видеть не': 5,\n",
       " 'виноватый из': 4,\n",
       " 'видеть один': 4,\n",
       " 'видеть крик': 4,\n",
       " 'ерунда ничто': 3,\n",
       " 'если подумать': 3,\n",
       " 'до гора': 3,\n",
       " 'публика дура': 3,\n",
       " 'истинный происшествие': 2,\n",
       " 'астрахань рассказывать': 2}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_couples_slice(zoshenko_couples, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'астроном обнаруживать': 3,\n",
       " 'астроном подводить': 2,\n",
       " 'комплексный наблюдение': 2,\n",
       " 'опубликовывать сайт': 2,\n",
       " 'существование звезда': 2,\n",
       " 'суперземля минимальный': 2,\n",
       " 'орбитальный период': 2,\n",
       " 'большой полуось': 2,\n",
       " 'орбита астрономический': 2,\n",
       " 'астрономический единица': 2}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_couples_slice(N_1_couples, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'анализ статистический': 3,\n",
       " 'горный высота': 2,\n",
       " 'большой данные': 2,\n",
       " 'использоваться случай': 2,\n",
       " 'оставаться связанный': 2,\n",
       " 'исследовать мнение': 2,\n",
       " 'официальный статистический': 2,\n",
       " 'использоваться разный': 2,\n",
       " 'отрасль литература': 2,\n",
       " 'источник черпать': 2}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_couples_slice(QGIS_couples, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'дикий гусь': 4,\n",
       " 'зимний солнцестояние': 3,\n",
       " 'конец лето': 3,\n",
       " 'до наш': 3,\n",
       " 'домашний гусь': 3,\n",
       " 'история рим': 3,\n",
       " 'конец конец': 3,\n",
       " 'который так': 3,\n",
       " 'город гусь': 3,\n",
       " 'осенний равноденствие': 2}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_couples_slice(Gus_couples, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
